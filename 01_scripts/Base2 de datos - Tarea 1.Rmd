---
title: "Bases de datos - Tarea 1"
output: html_document
date: "2025-10-24"
---

#0. Introducción
Este documento tiene como objetivo explicar el paso a paso de cómo se obtuvieron las bases que se trabajaron en la Tarea 1. En total se recuperaron dos bases de datos, ambas provenientes de Youtube y con fecha del 21 de octubre del 2025. La primera corresponde a la playlist titulada "Going Seventeen" del canal "Seventeen", mientras que la segunda es mi historial de reproducciones de Youtube, filtrada para que solo aparezca la información relacionada con "Going Seventeen". No les voy a mentir, con la ayuda de ChatGPT logré llegar al primero facilmente. Esto me llenó de un falso sentimiento de confianza que se destruyó (momentaneamente) al intentar (en múltiples oportunidades) llegar a la versión de mi historial que deseaba. Lo positivo es que no me rendí y se logró. Así que tengo la convicción de que si yo lo logré, tú seguramente también podrás! 
Este tutorial constará de dos partes para explicar por separado la obtención de las bases. Espero sea de ayuda. En caso de tener sugerencias de como optimizar el proceso, soy toda oídos.



# 1. Armar base de datos general de youtube
En este paso, se realizará una base de datos desde Youtube API que contenga los datos principales de la playlist de "Going Seventeen" del grupo musical de K-Pop Seventeen. Al ser primera vez que hacía algo como esto, exploré con ayuda de ChatGPT, detallo lo que me funcionó pues no todas sus recomendaciones fueron útiles.

### 1.0 Instalar y cargar librerías
Para esta primera base estos fueron los paquetes que utilicé, en caso de no tenerlos recuerda instalarlos con la función "install.packages".
```{r}
library(httr)
library(jsonlite)
library(dplyr)
```


## 1.1 Obtener y definir una API Key de Youtube
Primero, se debe tener una llave de acceso (API KEY), la cual te permitirá extraer los datos. Para ello, el primer paso es crear un proyecto en Google Cloud, por lo que es también es necesario tener una cuenta de Google. ¡No es necesario ingresar una tarjeta o pagar para hacerlo, es gratis! Dentro del proyecto se debe activar la Youtube Data API v3. Cuando esté activa, hay que ir a "APIs y servicios", especificamente "Credenciales". Ahí es: "Crear credenciales" -> "Clave de API". Se generará un código largo y random. Ese lo debes copiar pues ¡esa es tu API Key! Por último, ChatGPT me recomendó limitar su uso para evitar que alguien use mi clave con otra API de Google. Sonó razonable así que lo hice en "Restringir clave" -> seleccionar en "Restricciones de API" "Youtube Data API v3". 

Ahora que tenemos esto listo, podemos partir la magia (el código en RStudio). Para facilitar el resto de los códigos recomiendo definirla como un valor de la siguiente manera: 
```{r}
api_key <- "pega_aquí_tu_APIKey"
```


## 1.2 Definir las Playlist IDs
También es recomendable tener definido como valor en nuestro espacio el ID de la Playlist que se busca analizar, en este caso "Going Seventeen". El ID lo puedes encontrar al final del url de la playlist, como se explica a continuación:

```{r - Playlist IDs}
## ¿Cómo consigo el ID de la playlist? ¡Fácil!
### Link de la playlist de Going Seventeen: https://youtube.com/playlist?list=PLk_UmMfvZDx21Z9eEQ9DcIlUfZp1uwEup&si=j7UgNj3k3JOEPGA1
### ID de la playlist: PLk_UmMfvZDx21Z9eEQ9DcIlUfZp1uwE (es decir lo que está después de "list=" y antes de "up&si=")

#Con esto claro, lo definimos como valor:
playlist_2019_id <- "PLk_UmMfvZDx2-r7Kt-k2GjtQcTecKAB6p"
playlist_2020_id <- "PLk_UmMfvZDx1Ug2GQ5NCijKz7Q3pmZLlT"
playlist_2021_id <- "PLk_UmMfvZDx21Z9eEQ9DcIlUfZp1uwE"

```

## 1.3 Definimos las funciones
### 1.3.1 Obtener los videos de la playlist
Esta es una función para extraer todos los IDs de video asociados con esta playlist. Con esto podremos después obtener las estadísticas asociadas a cada uno de ellos. Esta función me la dio Grok y me funcionó muy bien con todos los videos, por lo que dejaré hasta sus comentarios explicativos
```{r}
#Definimos la función:
get_all_videos <- function(playlist_id, api_key) {
  base_url <- "https://www.googleapis.com/youtube/v3/playlistItems"
  all_items <- list()
  next_page_token <- NULL
  
  repeat {
    res <- GET(base_url, query = list(
      part = "contentDetails",
      playlistId = playlist_id,
      maxResults = 50,
      pageToken = next_page_token,
      key = api_key
    ))
    
    data <- fromJSON(rawToChar(res$content), flatten = TRUE)
    
    # Guardar items si existen
    if (!is.null(data$items)) {
      all_items <- append(all_items, list(data$items))
    }
    
    next_page_token <- data$nextPageToken
    if (is.null(next_page_token)) break
  }
  
  # Combinar todas las páginas
  all_df <- bind_rows(all_items)
  
  # Algunas veces la ruta es items$contentDetails.videoId o directamente contentDetails.videoId
  if ("contentDetails.videoId" %in% names(all_df)) {
    video_ids <- all_df$contentDetails.videoId
  } else if ("videoId" %in% names(all_df)) {
    video_ids <- all_df$videoId
  } else {
    stop("No se encontró la columna con los videoId en la respuesta de la API.")
  }
  
  tibble(video_id = video_ids)
}

```

### 1.3.2 Obtener las estadísticas de los videos
Creamos otra función que nos permita obtener los datos relacionados con las vistas, likes y comentarios.
```{r}
#Definimos la función
get_video_stats <- function(video_id, api_key) {
  url <- "https://www.googleapis.com/youtube/v3/videos"
  res <- GET(url, query = list(
    part = "snippet,statistics",
    id = video_id,
    key = api_key
  ))
  
  info <- fromJSON(rawToChar(res$content))
  
  if (length(info$items) == 0) return(NULL)
  
  tibble(
    video_id = video_id,
    titulo = info$items$snippet$title,
    fecha = info$items$snippet$publishedAt,
    vistas = as.numeric(info$items$statistics$viewCount),
    likes = as.numeric(info$items$statistics$likeCount),
    comentarios = as.numeric(info$items$statistics$commentCount)
  )
}

```


## 1.4 Ejecutamos
```{r}
# Obtener videos
videos_2019 <- get_all_videos(playlist_2019_id, api_key)
videos_2020 <- get_all_videos(playlist_2020_id, api_key)
videos_2021 <- get_all_videos(playlist_2021_id, api_key)

# Obtener estadísticas
playlist_2019_df <- map_dfr(videos_2019$video_id, get_video_stats, api_key = api_key) %>%
  mutate(playlist = "2019")

playlist_2020_df <- map_dfr(videos_2020$video_id, get_video_stats, api_key = api_key) %>%
  mutate(playlist = "2020")

playlist_2021_df <- map_dfr(videos_2021$video_id, get_video_stats, api_key = api_key) %>%
  mutate(playlist = "2021")

```

## 1.5 Revisamos, seleccionamos variables y limpiamos los títulos
Revisamos las variables (todas en español), seleccionamos las que queremos (en mi caso, video_id, titulo, vistas, likes y comentarios)
```{r}
colnames(playlist_2019_df)
colnames(playlist_2020_df)
colnames(playlist_2021_df)

# Seleccionamos solo bases de interés
playlist_2019_df <- playlist_2019_df |> 
  select(c(video_id, titulo, vistas, likes, comentarios))

playlist_2020_df <- playlist_2020_df |> 
  select(c(video_id, titulo, vistas, likes, comentarios))

playlist_2021_df <- playlist_2021_df |> 
  select(c(video_id, titulo, vistas, likes, comentarios))

#Limpiamos los nombres de los títulos
playlist_2019_df <- playlist_2019_df |> 
  mutate(titulo = str_replace(titulo, fixed("[GOING SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2020"), "")) |>
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2019"), "")) |> 
  mutate(titulo = str_replace_all(titulo, "[\uAC00-\uD7A3]", "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[]"), "")) |> 
  mutate(titulo = str_squish(titulo))

playlist_2020_df <- playlist_2020_df |> 
  mutate(titulo = str_replace(titulo, fixed("[GOING SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2020"), "")) |>
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2019"), "")) |> 
  mutate(titulo = str_replace_all(titulo, "[\uAC00-\uD7A3]", "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[]"), "")) |> 
  mutate(titulo = str_squish(titulo))

playlist_2021_df <- playlist_2021_df |> 
  mutate(titulo = str_replace(titulo, fixed("[GOING SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2020"), "")) |>
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2019"), "")) |> 
  mutate(titulo = str_replace_all(titulo, "[\uAC00-\uD7A3]", "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[]"), "")) |> 
  mutate(titulo = str_squish(titulo))

```

# 1.6 Unimos todas las bases en una sola y exportamos
Ahora bien, ¿podrían unirse las bases y después limpiar los nombres? Sip, seguramente que si y seguramente tuve que haberlo hecho así, pero no lo hice y me di cuenta en este paso. Aquí finalmente, uniremos las tres bases en una súper gran base única y la exportaremos.
```{r}
playlist_total_df <- bind_rows(playlist_2019_df, playlist_2020_df, playlist_2021_df)

#Exportamos base como CSV
write.csv(playlist_total_df, "youtube_playlist_GOSE_stats_complete.csv", row.names = FALSE)
```


# 2. Armar base de datos de historial personal
Al tener la base de datos armada con la playlist, queda realizar la base de datos del historial personal. Nuevamente, me apoyé de ChatGPT y Grok en este proceso, fue trabajo conjunto.

## 2.0 Instalar y cargar librerias
Para esta segunda base estos fueron los paquetes que utilicé, en caso de no tenerlos recuerda instalarlos con la función "install.packages".
```{r}
library(jsonlite)
library(dplyr)
library(stringr)
library(lubridate)
```


## 2.1 Exportar desde Google Takeout historial de visualización
Basicamente desde Google Takeout uno puede exportar diversos datos relacionados con su cuenta de Google, entre ellos los de Youtube. En este caso, decidí exportar distintos datos, incluído el historial de videos, el cual incluye historial de búsqueda e historial de reproducciones. Según el tamaño y la cantidad de los datos se demora cierto tiempo, lo cual es un factor a considerar para realizar este tipo de análisis.Mil veces recomendado exportarlo en JSON y no en HTML, tuve muchos fracasos con este último y Google Takeout te da esa opción, por lo que escogí después la felicidad (que el código me corriera bien)

## 2.2 Leer el archivo JSON del historial de reproducciones y verificar columnas
Empezamos en RStudio leyendo el archivo en JSON, verificando las columnas y seleccionando las columnas que ocuparemos. En este caso, yo quería tener el título de cada video, la URL (para sacar después los "video_id") y el tiempo (para formatear después la fecha en que lo vi).
```{r}
# Leer archivo JSON
json_file <- "C:/Users/Marita Abbott/Documents/R/R Magister/Tareas/Tarea 1/historial_de_reproducciones.json"
watch_history <- fromJSON(json_file, flatten = TRUE)

#Verificar columnas
cat("Columnas en watch_history:\n")
print(colnames(watch_history))

# Convertir a data frame y seleccionar columnas relevantes
watch_df <- as.data.frame(watch_history) %>%
  select(title, titleUrl, time) %>%
  rename(watch_date = time)

# Verificar las columnas y primeras filas
cat("Columnas en watch_df antes de extraer video_id:\n")
print(colnames(watch_df))
cat("Primeras filas de watch_df:\n")
head(watch_df)
```

## 2.3 Extraer IDs de videos desde los URLs
Al estar disponible solamente los URLs, extraemos desde ahí los "video_id" de cada video con el fin de filtrar por esa variable los videos de la playlist de Going Seventeen más adelante.
```{r}
#Extraer video_id de titleUrl
watch_df <- watch_df %>%
  mutate(video_id = str_extract(titleUrl, "(?<=v=)[^&]+")) %>%
  filter(!is.na(video_id) & video_id != "")  # Eliminar entradas sin video_id

# Verificar que video_id se creó correctamente
cat("Columnas en watch_df después de extraer video_id:\n")
print(colnames(watch_df))
cat("Primeras filas de watch_df con video_id:\n")
head(watch_df)
cat("Valores únicos de video_id en watch_df (primeros 10):\n")
print(head(unique(watch_df$video_id), 10))
```

## 2.4 Limpiar el título (quitar el "Has visto" que aparece en todos los títulos)
En el historial se incluye un "Has visto" antes de cada observación. Me molestaba así que se lo saqué, honestamente lo recomiendo.
```{r}
#Limpiar el título (quitar "Has visto ")
watch_df <- watch_df %>%
  mutate(title = gsub("^Has visto ", "", title)) |> 
  rename(titulo = title, fecha_vista = watch_date)
```

## 2.5 Leer el archivo CSV de la playlist general que hicimos anteriormente
Como necesitamos los "video_id" (y esto me tomó más de un día), leemos el archivo CSV de la playlist general que creamos anteriormente.
```{r}
#Leer el archivo CSV de la playlist
playlist_file <- "C:/Users/Marita Abbott/Documents/R/R Magister/Tareas/Tarea 1/youtube_playlist_GOSE_stats_complete.csv"
playlist_df <- read.csv(playlist_file, stringsAsFactors = FALSE)

# Verificar nombres de columnas en playlist_df
cat("Columnas en playlist_df:\n")
print(colnames(playlist_df))
cat("Primeras filas de playlist_df:\n")
head(playlist_df)

# Verificar que video_id existe en playlist_df
cat("Columnas en playlist_df después de renombrar:\n")
print(colnames(playlist_df))
cat("Valores únicos de video_id en playlist_df (primeros 10):\n")
print(head(unique(playlist_df$video_id), 10))

```

## 2.6 Filtrar historial para conservar solo videos de la playlist de Going Seventeen
En este paso, utilizamos los video_id de la playlist para filtrar el historial general de videos que tenemos para que queden solo los que coincidan entre si.
```{r}
#Filtrar historial para conservar solo videos de la playlist
final_df <- watch_df %>%
  filter(video_id %in% playlist_df$video_id) %>%
  select(titulo, video_id, fecha_vista)

```

## 2.7 Formatear la fecha
Se cambia el formato en el que aparece la fecha y creamos la columna de "fecha_vista". Es decir, la fecha (año-mes-día) en la que vi el video.
```{r}
#Formatear la fecha (solo YYYY-MM-DD)
final_df <- final_df %>%
  mutate(fecha_vista = as.Date(ymd_hms(fecha_vista)))

# Verificar el resultado
cat("Primeras filas del resultado final:\n")
head(final_df)
cat("Número de filas en el resultado final:", nrow(final_df), "\n")

```

## 2.8 Limpieza de títulos
Repetimos el procedimiento de limpieza realizado en la base de datos de la playlist general para así ahorrarnos problemas en el futuro y que estas coincidan desde un inicio.
```{r}
final_df <- final_df |> 
  mutate(titulo = str_replace(titulo, fixed("[GOING SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[SEVENTEEN]"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2020"), "")) |>
  mutate(titulo = str_replace(titulo, fixed("GOING SEVENTEEN 2019"), "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[GOING SEVENTEEN SPECIAL]"), "")) |> 
  mutate(titulo = str_replace_all(titulo, "[\uAC00-\uD7A3]", "")) |> 
  mutate(titulo = str_replace(titulo, fixed("[]"), "")) |> 
  mutate(titulo = str_squish(titulo))
```


## 2.8 Guardar y exportar
Solo nos queda guardarla en nuestro dispositivo y listo.
```{r}

#Guardar en CSV
write.csv(final_df, "../02_data/historial_personal_gose.csv", row.names = FALSE)
cat("Archivo guardado como 'historial_personal_gose.csv'\n")

```


# 3. ¡LISTO! Podemos comenzar a hacer nuestros análisis.
Finalmente, podemos ser felices. 
